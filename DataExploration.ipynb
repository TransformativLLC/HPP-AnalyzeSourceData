{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:14:34.940544Z",
     "start_time": "2025-08-12T02:14:34.938640Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ],
   "id": "ad0e43886ca794f0",
   "outputs": [],
   "execution_count": 23
  },
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T01:36:07.005449Z",
     "start_time": "2025-08-12T01:36:06.996095Z"
    }
   },
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv();"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T01:36:09.303224Z",
     "start_time": "2025-08-12T01:36:08.669408Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "from source_file_utils import get_source_file_names"
   ],
   "id": "c92137384f960577",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:04:47.142451Z",
     "start_time": "2025-08-12T02:04:47.099161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "from functools import wraps\n",
    "import re\n",
    "from typing import Any, Callable, Dict, List, Mapping, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pydantic import BaseModel, Field, field_validator\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Exceptions\n",
    "# ---------------------------\n",
    "\n",
    "class AnalyzerError(RuntimeError):\n",
    "    \"\"\"Base class for analyzer errors.\"\"\"\n",
    "\n",
    "\n",
    "class ConfigurationError(AnalyzerError):\n",
    "    \"\"\"Raised for invalid analyzer configuration.\"\"\"\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Configuration & Reports\n",
    "# ---------------------------\n",
    "\n",
    "class ForeignKeySpec(BaseModel):\n",
    "    \"\"\"Declare a foreign key relationship for referential integrity checking.\"\"\"\n",
    "    child_table: str = Field(..., description=\"Table with the foreign key.\")\n",
    "    child_column: str = Field(..., description=\"Column in child that references a parent key.\")\n",
    "    parent_table: str = Field(..., description=\"Referenced table.\")\n",
    "    parent_column: str = Field(..., description=\"Primary/unique key column in the parent table.\")\n",
    "\n",
    "\n",
    "class AnalyzerConfig(BaseModel):\n",
    "    \"\"\"Configuration for the DataQualityAnalyzer.\"\"\"\n",
    "    sample_rows: Optional[int] = Field(\n",
    "        None,\n",
    "        description=\"Row count to sample per table for expensive checks. None = full table.\"\n",
    "    )\n",
    "    duplicate_key_hints: List[str] = Field(\n",
    "        default_factory=lambda: [\"id\", \"code\", \"number\", \"email\", \"sku\", \"name\"],\n",
    "        description=\"Substrings used to discover likely identifier-like columns.\"\n",
    "    )\n",
    "    string_format_checks: List[str] = Field(\n",
    "        default_factory=lambda: [\"date\", \"int_like\", \"float_like\", \"email\", \"phone\", \"uuid\", \"iso_datetime\", \"zip\"],\n",
    "        description=\"String format classifiers to run.\"\n",
    "    )\n",
    "    outlier_method: str = Field(\n",
    "        default=\"iqr\",\n",
    "        description=\"Numeric outlier method: 'iqr' or 'zscore'.\"\n",
    "    )\n",
    "    z_threshold: float = Field(default=3.5, ge=0)\n",
    "    iqr_multiplier: float = Field(default=1.5, ge=0)\n",
    "    max_top_values: int = Field(default=10, ge=1)\n",
    "    fk_specs: List[ForeignKeySpec] = Field(default_factory=list)\n",
    "\n",
    "    @field_validator(\"outlier_method\")\n",
    "    def _validate_outlier_method(cls, v: str) -> str:\n",
    "        v = v.lower()\n",
    "        if v not in {\"iqr\", \"zscore\"}:\n",
    "            raise ConfigurationError(\"outlier_method must be 'iqr' or 'zscore'\")\n",
    "        return v\n",
    "\n",
    "\n",
    "class ColumnProfile(BaseModel):\n",
    "    column: str\n",
    "    dtype: str\n",
    "    non_null: int\n",
    "    null_pct: float\n",
    "    unique: int\n",
    "    constant: bool\n",
    "    avg_len: Optional[float] = None\n",
    "    pct_empty_str: Optional[float] = None\n",
    "    pct_whitespace_str: Optional[float] = None\n",
    "    min_val: Optional[float] = None\n",
    "    max_val: Optional[float] = None\n",
    "    example_values: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class ColumnIssues(BaseModel):\n",
    "    column: str\n",
    "    has_mixed_types: bool = False\n",
    "    detected_formats: Dict[str, float] = Field(default_factory=dict)  # format -> coverage %\n",
    "    case_variants: Optional[List[str]] = None  # for likely categories\n",
    "\n",
    "\n",
    "class DuplicateReport(BaseModel):\n",
    "    exact_row_duplicates: int\n",
    "    near_text_duplicates: Dict[str, int]  # column -> count (normalized by stripping/punct/case)\n",
    "    candidate_key_dupes: Dict[str, int]   # column -> count of duplicate keys\n",
    "\n",
    "\n",
    "class OutlierReport(BaseModel):\n",
    "    method: str\n",
    "    counts_by_column: Dict[str, int]  # numeric column -> outlier count\n",
    "\n",
    "\n",
    "class FKIssue(BaseModel):\n",
    "    spec: ForeignKeySpec\n",
    "    missing_fk_count: int\n",
    "\n",
    "\n",
    "class TableReport(BaseModel):\n",
    "    table: str\n",
    "    rows: int\n",
    "    cols: int\n",
    "    column_profiles: List[ColumnProfile]\n",
    "    column_issues: List[ColumnIssues]\n",
    "    duplicates: DuplicateReport\n",
    "    outliers: OutlierReport\n",
    "    notes: List[str] = Field(default_factory=list)\n",
    "\n",
    "\n",
    "class AnalysisResult(BaseModel):\n",
    "    summary: pd.DataFrame  # table-level summary\n",
    "    tables: Dict[str, TableReport]\n",
    "    fk_issues: List[FKIssue] = Field(default_factory=list)\n",
    "    model_config = {'arbitrary_types_allowed': True}\n",
    "\n",
    "    def to_excel(self, path: str) -> None:\n",
    "        \"\"\"Write the full analysis as a multi-sheet Excel workbook.\"\"\"\n",
    "        with pd.ExcelWriter(path, engine=\"xlsxwriter\") as xw:\n",
    "            self.summary.to_excel(xw, sheet_name=\"summary\", index=False)\n",
    "            for tname, trep in self.tables.items():\n",
    "                # profiles\n",
    "                pd.DataFrame([p.model_dump() for p in trep.column_profiles]).to_excel(\n",
    "                    xw, sheet_name=f\"{tname[:25]}_profiles\", index=False\n",
    "                )\n",
    "                # issues\n",
    "                pd.DataFrame([i.model_dump() for i in trep.column_issues]).to_excel(\n",
    "                    xw, sheet_name=f\"{tname[:25]}_issues\", index=False\n",
    "                )\n",
    "                # duplicates\n",
    "                dup_df = pd.DataFrame(\n",
    "                    {\n",
    "                        \"metric\": [\"exact_row_duplicates\"],\n",
    "                        \"value\": [trep.duplicates.exact_row_duplicates],\n",
    "                    }\n",
    "                )\n",
    "                if trep.duplicates.near_text_duplicates:\n",
    "                    dup_df = pd.concat(\n",
    "                        [\n",
    "                            dup_df,\n",
    "                            pd.DataFrame(\n",
    "                                [\n",
    "                                    {\"metric\": f\"near_text_dupes:{c}\", \"value\": v}\n",
    "                                    for c, v in trep.duplicates.near_text_duplicates.items()\n",
    "                                ]\n",
    "                            ),\n",
    "                        ],\n",
    "                        ignore_index=True,\n",
    "                    )\n",
    "                if trep.duplicates.candidate_key_dupes:\n",
    "                    dup_df = pd.concat(\n",
    "                        [\n",
    "                            dup_df,\n",
    "                            pd.DataFrame(\n",
    "                                [\n",
    "                                    {\"metric\": f\"candidate_key_dupes:{c}\", \"value\": v}\n",
    "                                    for c, v in trep.duplicates.candidate_key_dupes.items()\n",
    "                                ]\n",
    "                            ),\n",
    "                        ],\n",
    "                        ignore_index=True,\n",
    "                    )\n",
    "                dup_df.to_excel(xw, sheet_name=f\"{tname[:25]}_dupes\", index=False)\n",
    "\n",
    "                # outliers\n",
    "                out_df = pd.DataFrame(\n",
    "                    [{\"column\": c, \"outlier_count\": n, \"method\": trep.outliers.method}\n",
    "                     for c, n in trep.outliers.counts_by_column.items()]\n",
    "                )\n",
    "                out_df.to_excel(xw, sheet_name=f\"{tname[:25]}_outliers\", index=False)\n",
    "\n",
    "            if self.fk_issues:\n",
    "                pd.DataFrame(\n",
    "                    [\n",
    "                        {\n",
    "                            \"child_table\": i.spec.child_table,\n",
    "                            \"child_column\": i.spec.child_column,\n",
    "                            \"parent_table\": i.spec.parent_table,\n",
    "                            \"parent_column\": i.spec.parent_column,\n",
    "                            \"missing_fk_count\": i.missing_fk_count,\n",
    "                        }\n",
    "                        for i in self.fk_issues\n",
    "                    ]\n",
    "                ).to_excel(xw, sheet_name=\"fk_issues\", index=False)\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Check registry (pluggable)\n",
    "# ---------------------------\n",
    "\n",
    "CheckFn = Callable[[str, pd.DataFrame, AnalyzerConfig], Dict[str, Any]]\n",
    "_CHECKS: List[CheckFn] = []\n",
    "\n",
    "\n",
    "def register_check(fn: CheckFn) -> CheckFn:\n",
    "    \"\"\"Decorator to register a table-level check producing a dict payload.\"\"\"\n",
    "    @wraps(fn)\n",
    "    def _wrapped(table: str, df: pd.DataFrame, cfg: AnalyzerConfig) -> Dict[str, Any]:\n",
    "        return fn(table, df, cfg)\n",
    "    _CHECKS.append(_wrapped)\n",
    "    return _wrapped\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Helpers\n",
    "# ---------------------------\n",
    "\n",
    "_STRIP_PUNCT_RE = re.compile(r\"[^\\w\\s]\", flags=re.UNICODE)\n",
    "\n",
    "\n",
    "def _maybe_sample(df: pd.DataFrame, n: Optional[int]) -> pd.DataFrame:\n",
    "    if n is None or len(df) <= n:\n",
    "        return df\n",
    "    return df.sample(n=n, random_state=42)\n",
    "\n",
    "\n",
    "def _try_parse_datetime(s: pd.Series) -> pd.Series:\n",
    "    return pd.to_datetime(s, errors=\"coerce\")\n",
    "\n",
    "\n",
    "def _is_int_like(s: pd.Series) -> pd.Series:\n",
    "    return s.str.fullmatch(r\"[+-]?\\d+\", na=False)\n",
    "\n",
    "\n",
    "def _is_float_like(s: pd.Series) -> pd.Series:\n",
    "    return s.str.fullmatch(r\"[+-]?(\\d+(\\.\\d+)?|\\.\\d+)\", na=False)\n",
    "\n",
    "\n",
    "def _is_email(s: pd.Series) -> pd.Series:\n",
    "    return s.str.fullmatch(r\"[^@\\s]+@[^@\\s]+\\.[^@\\s]+\", na=False)\n",
    "\n",
    "\n",
    "def _is_phone(s: pd.Series) -> pd.Series:\n",
    "    return s.str.fullmatch(r\"(\\+?\\d{1,3}[\\s.-]?)?\\(?\\d{2,4}\\)?[\\s.-]?\\d{3,4}[\\s.-]?\\d{3,4}\", na=False)\n",
    "\n",
    "\n",
    "def _is_uuid(s: pd.Series) -> pd.Series:\n",
    "    return s.str.fullmatch(r\"[0-9a-fA-F]{8}-[0-9a-fA-F]{4}-[1-5][0-9a-fA-F]{3}-[89abAB][0-9a-fA-F]{3}-[0-9a-fA-F]{12}\", na=False)\n",
    "\n",
    "\n",
    "def _is_iso_datetime(s: pd.Series) -> pd.Series:\n",
    "    return s.str.fullmatch(r\"\\d{4}-\\d{2}-\\d{2}(T|\\s)\\d{2}:\\d{2}(:\\d{2})?(\\.\\d+)?(Z|[+-]\\d{2}:\\d{2})?\", na=False)\n",
    "\n",
    "\n",
    "def _is_zip(s: pd.Series) -> pd.Series:\n",
    "    return s.str.fullmatch(r\"\\d{5}(-\\d{4})?\", na=False)\n",
    "\n",
    "\n",
    "_FORMATTERS: Dict[str, Callable[[pd.Series], pd.Series]] = {\n",
    "    \"date\": lambda s: _try_parse_datetime(s).notna(),\n",
    "    \"int_like\": _is_int_like,\n",
    "    \"float_like\": _is_float_like,\n",
    "    \"email\": _is_email,\n",
    "    \"phone\": _is_phone,\n",
    "    \"uuid\": _is_uuid,\n",
    "    \"iso_datetime\": _is_iso_datetime,\n",
    "    \"zip\": _is_zip,\n",
    "}\n",
    "\n",
    "\n",
    "def _normalize_text_for_dupe(s: pd.Series) -> pd.Series:\n",
    "    s2 = s.astype(str).str.lower().str.strip()\n",
    "    s2 = s2.str.replace(_STRIP_PUNCT_RE, \"\", regex=True).str.replace(r\"\\s+\", \" \", regex=True)\n",
    "    return s2\n",
    "\n",
    "\n",
    "def _iqr_outliers(x: pd.Series, k: float = 1.5) -> pd.Series:\n",
    "    q1 = x.quantile(0.25)\n",
    "    q3 = x.quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    low, high = q1 - k * iqr, q3 + k * iqr\n",
    "    return (x < low) | (x > high)\n",
    "\n",
    "\n",
    "def _zscore_outliers(x: pd.Series, z: float = 3.5) -> pd.Series:\n",
    "    mu = x.mean()\n",
    "    sigma = x.std(ddof=0)\n",
    "    if sigma == 0 or np.isnan(sigma):\n",
    "        return pd.Series(False, index=x.index)\n",
    "    return (np.abs((x - mu) / sigma) > z)\n",
    "\n",
    "\n",
    "def _top_examples(s: pd.Series, k: int) -> List[str]:\n",
    "    vc = s.dropna().astype(str).head(10000).value_counts(dropna=False)\n",
    "    return [str(v) for v in vc.head(k).index.tolist()]\n",
    "\n",
    "\n",
    "def _likely_identifier_columns(df: pd.DataFrame, hints: List[str]) -> List[str]:\n",
    "    cols = []\n",
    "    for c in df.columns:\n",
    "        lc = c.lower()\n",
    "        if any(h in lc for h in hints):\n",
    "            cols.append(c)\n",
    "    return cols\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Checks\n",
    "# ---------------------------\n",
    "\n",
    "@register_check\n",
    "def profile_columns(table: str, df: pd.DataFrame, cfg: AnalyzerConfig) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Column-level profiling: dtype, missingness, unique counts, constant columns,\n",
    "    numeric min/max, string length stats, empty/whitespace ratio, examples.\n",
    "    \"\"\"\n",
    "    profiles: List[ColumnProfile] = []\n",
    "    sample = _maybe_sample(df, cfg.sample_rows)\n",
    "\n",
    "    for c in df.columns:\n",
    "        s = sample[c]\n",
    "        dtype = str(df[c].dtype)\n",
    "        non_null = int(s.notna().sum())\n",
    "        null_pct = float((s.isna().mean() * 100.0))\n",
    "        unique = int(s.nunique(dropna=True))\n",
    "        constant = unique <= 1\n",
    "\n",
    "        avg_len = pct_empty = pct_ws = None\n",
    "        min_val = max_val = None\n",
    "\n",
    "        if pd.api.types.is_numeric_dtype(df[c]):\n",
    "            min_val = float(pd.to_numeric(s, errors=\"coerce\").min(skipna=True)) if non_null else None\n",
    "            max_val = float(pd.to_numeric(s, errors=\"coerce\").max(skipna=True)) if non_null else None\n",
    "        elif pd.api.types.is_string_dtype(df[c]) or df[c].dtype == \"object\":\n",
    "            s_str = s.dropna().astype(str)\n",
    "            if not s_str.empty:\n",
    "                lengths = s_str.str.len()\n",
    "                avg_len = float(lengths.mean())\n",
    "                pct_empty = float((s_str == \"\").mean() * 100.0)\n",
    "                pct_ws = float((s_str.str.fullmatch(r\"\\s*\").mean()) * 100.0)\n",
    "\n",
    "        profiles.append(\n",
    "            ColumnProfile(\n",
    "                column=c,\n",
    "                dtype=dtype,\n",
    "                non_null=non_null,\n",
    "                null_pct=round(null_pct, 3),\n",
    "                unique=unique,\n",
    "                constant=constant,\n",
    "                avg_len=avg_len,\n",
    "                pct_empty_str=pct_empty,\n",
    "                pct_whitespace_str=pct_ws,\n",
    "                min_val=min_val,\n",
    "                max_val=max_val,\n",
    "                example_values=_top_examples(s, cfg.max_top_values),\n",
    "            )\n",
    "        )\n",
    "    return {\"column_profiles\": profiles}\n",
    "\n",
    "\n",
    "@register_check\n",
    "def detect_type_and_format_issues(table: str, df: pd.DataFrame, cfg: AnalyzerConfig) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Detect mixed types in object-like columns and estimate format coverage (emails, phones, dates, etc.).\n",
    "    Also report case variants for likely categorical fields.\n",
    "    \"\"\"\n",
    "    issues: List[ColumnIssues] = []\n",
    "    sample = _maybe_sample(df, cfg.sample_rows)\n",
    "\n",
    "    for c in df.columns:\n",
    "        s = sample[c]\n",
    "        col_issue = ColumnIssues(column=c)\n",
    "\n",
    "        # Mixed types detection\n",
    "        if df[c].dtype == \"object\":\n",
    "            pytypes = s.dropna().map(type)\n",
    "            if not pytypes.empty:\n",
    "                counts = pytypes.value_counts()\n",
    "                col_issue.has_mixed_types = counts.shape[0] > 1\n",
    "\n",
    "        # String format checks\n",
    "        if pd.api.types.is_string_dtype(df[c]) or df[c].dtype == \"object\":\n",
    "            s_str = s.dropna().astype(str)\n",
    "            detected: Dict[str, float] = {}\n",
    "            for name in cfg.string_format_checks:\n",
    "                fn = _FORMATTERS.get(name)\n",
    "                if fn and not s_str.empty:\n",
    "                    m = fn(s_str)\n",
    "                    detected[name] = round(float(m.mean() * 100.0), 2)\n",
    "            col_issue.detected_formats = detected\n",
    "\n",
    "            # Case-variant categories (heuristic): low-cardinality textual columns\n",
    "            nunique = s_str.nunique(dropna=True)\n",
    "            if 1 < nunique <= 50:\n",
    "                lower_map = s_str.str.strip().str.lower().value_counts()\n",
    "                if lower_map.shape[0] < nunique:\n",
    "                    # same tokens differing by case/space exist\n",
    "                    col_issue.case_variants = _top_examples(s_str, cfg.max_top_values)\n",
    "\n",
    "        issues.append(col_issue)\n",
    "\n",
    "    return {\"column_issues\": issues}\n",
    "\n",
    "\n",
    "@register_check\n",
    "def duplicate_scans(table: str, df: pd.DataFrame, cfg: AnalyzerConfig) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Exact row duplicates, normalized near-duplicates for text columns,\n",
    "    and duplicate counts for likely identifier-like columns.\n",
    "    \"\"\"\n",
    "    sample = _maybe_sample(df, cfg.sample_rows)\n",
    "    # Exact row duplicates\n",
    "    exact_dupes = int(sample.duplicated().sum())\n",
    "\n",
    "    # Near text duplicates per column (case/space/punct normalized)\n",
    "    near_text_dupes: Dict[str, int] = {}\n",
    "    for c in sample.columns:\n",
    "        if pd.api.types.is_string_dtype(df[c]) or df[c].dtype == \"object\":\n",
    "            s_norm = _normalize_text_for_dupe(sample[c].fillna(\"\"))\n",
    "            dups = int(s_norm.duplicated().sum())\n",
    "            if dups > 0:\n",
    "                near_text_dupes[c] = dups\n",
    "\n",
    "    # Candidate key dupes\n",
    "    candidate_key_dupes: Dict[str, int] = {}\n",
    "    for c in _likely_identifier_columns(df, cfg.duplicate_key_hints):\n",
    "        dups = int(sample[c].duplicated(keep=False).sum()) if c in sample.columns else 0\n",
    "        if dups > 0:\n",
    "            candidate_key_dupes[c] = dups\n",
    "\n",
    "    return {\n",
    "        \"duplicates\": DuplicateReport(\n",
    "            exact_row_duplicates=exact_dupes,\n",
    "            near_text_duplicates=near_text_dupes,\n",
    "            candidate_key_dupes=candidate_key_dupes,\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "@register_check\n",
    "def numeric_outliers(table: str, df: pd.DataFrame, cfg: AnalyzerConfig) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Count numeric outliers per column using configured method.\n",
    "    \"\"\"\n",
    "    sample = _maybe_sample(df, cfg.sample_rows)\n",
    "    numeric_cols = [c for c in df.columns if pd.api.types.is_numeric_dtype(df[c])]\n",
    "    counts: Dict[str, int] = {}\n",
    "    for c in numeric_cols:\n",
    "        x = pd.to_numeric(sample[c], errors=\"coerce\").dropna()\n",
    "        if x.empty:\n",
    "            counts[c] = 0\n",
    "            continue\n",
    "        if cfg.outlier_method == \"iqr\":\n",
    "            m = _iqr_outliers(x, cfg.iqr_multiplier)\n",
    "        else:\n",
    "            m = _zscore_outliers(x, cfg.z_threshold)\n",
    "        counts[c] = int(m.sum())\n",
    "\n",
    "    return {\"outliers\": OutlierReport(method=cfg.outlier_method, counts_by_column=counts)}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Analyzer\n",
    "# ---------------------------\n",
    "\n",
    "class DataQualityAnalyzer:\n",
    "    \"\"\"Execute a suite of data-quality checks against pandas DataFrames.\"\"\"\n",
    "\n",
    "    def __init__(self, config: Optional[AnalyzerConfig] = None) -> None:\n",
    "        self.config = config or AnalyzerConfig()\n",
    "\n",
    "    def analyze(\n",
    "        self,\n",
    "        tables: Mapping[str, pd.DataFrame],\n",
    "    ) -> AnalysisResult:\n",
    "        \"\"\"Run all registered checks on each table and build an aggregate report.\n",
    "\n",
    "        Args:\n",
    "            tables: Mapping of table name -> DataFrame.\n",
    "\n",
    "        Returns:\n",
    "            AnalysisResult containing table-level reports, summary, and FK issues (if any).\n",
    "        \"\"\"\n",
    "        table_reports: Dict[str, TableReport] = {}\n",
    "        summary_rows: List[Dict[str, Any]] = []\n",
    "\n",
    "        # Per-table checks\n",
    "        for tname, df in tables.items():\n",
    "            payload: Dict[str, Any] = {}\n",
    "            for check in _CHECKS:\n",
    "                payload.update(check(tname, df, self.config))\n",
    "\n",
    "            profiles: List[ColumnProfile] = payload[\"column_profiles\"]\n",
    "            issues: List[ColumnIssues] = payload[\"column_issues\"]\n",
    "            dupes: DuplicateReport = payload[\"duplicates\"]\n",
    "            outliers: OutlierReport = payload[\"outliers\"]\n",
    "\n",
    "            report = TableReport(\n",
    "                table=tname,\n",
    "                rows=int(df.shape[0]),\n",
    "                cols=int(df.shape[1]),\n",
    "                column_profiles=profiles,\n",
    "                column_issues=issues,\n",
    "                duplicates=dupes,\n",
    "                outliers=outliers,\n",
    "            )\n",
    "            table_reports[tname] = report\n",
    "\n",
    "            summary_rows.append(\n",
    "                {\n",
    "                    \"table\": tname,\n",
    "                    \"rows\": report.rows,\n",
    "                    \"cols\": report.cols,\n",
    "                    \"exact_row_dupes\": dupes.exact_row_duplicates,\n",
    "                    \"columns_with_near_text_dupes\": len(dupes.near_text_duplicates),\n",
    "                    \"columns_with_candidate_key_dupes\": len(dupes.candidate_key_dupes),\n",
    "                    \"numeric_columns_with_outliers\": sum(1 for v in outliers.counts_by_column.values() if v > 0),\n",
    "                    \"avg_null_pct\": np.mean([p.null_pct for p in profiles]) if profiles else np.nan,\n",
    "                    \"constant_columns\": sum(1 for p in profiles if p.constant),\n",
    "                }\n",
    "            )\n",
    "\n",
    "        # Optional FK checks across tables\n",
    "        fk_issues: List[FKIssue] = []\n",
    "        if self.config.fk_specs:\n",
    "            for spec in self.config.fk_specs:\n",
    "                if spec.child_table not in tables or spec.parent_table not in tables:\n",
    "                    continue\n",
    "                child = tables[spec.child_table]\n",
    "                parent = tables[spec.parent_table]\n",
    "                if spec.child_column not in child.columns or spec.parent_column not in parent.columns:\n",
    "                    continue\n",
    "                child_keys = pd.Series(child[spec.child_column]).dropna().unique()\n",
    "                parent_keys = pd.Series(parent[spec.parent_column]).dropna().unique()\n",
    "                missing = ~pd.Series(child[spec.child_column]).isin(parent_keys)\n",
    "                missing_count = int(missing.sum())\n",
    "                if missing_count > 0:\n",
    "                    fk_issues.append(FKIssue(spec=spec, missing_fk_count=missing_count))\n",
    "\n",
    "        summary_df = pd.DataFrame(summary_rows).sort_values([\"rows\", \"table\"], ascending=[False, True], ignore_index=True)\n",
    "        return AnalysisResult(summary=summary_df, tables=table_reports, fk_issues=fk_issues)"
   ],
   "id": "344a016061cbd507",
   "outputs": [],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T01:36:11.237117Z",
     "start_time": "2025-08-12T01:36:11.233919Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 source files retrieved.\n"
     ]
    }
   ],
   "execution_count": 3,
   "source": [
    "source_file_names = get_source_file_names()\n",
    "print(f\"{len(source_file_names)} source files retrieved.\")"
   ],
   "id": "b513a36b17a8c049"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:17:06.595463Z",
     "start_time": "2025-08-12T02:17:05.130685Z"
    }
   },
   "cell_type": "code",
   "source": [
    "i = 3\n",
    "df = pd.read_excel(source_file_names[i])"
   ],
   "id": "1e3acf0c3aea79ce",
   "outputs": [],
   "execution_count": 29
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-08-12T02:17:09.295701Z",
     "start_time": "2025-08-12T02:17:07.358361Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# Create config with defaults â€” no foreign keys\n",
    "cfg = AnalyzerConfig(\n",
    "    sample_rows=None,        # None = run checks on all rows\n",
    "    outlier_method=\"iqr\",    # can be \"iqr\" or \"zscore\"\n",
    ")\n",
    "\n",
    "# Initialize analyzer\n",
    "analyzer = DataQualityAnalyzer(cfg)\n",
    "\n",
    "# Analyze the single table (name it anything you like)\n",
    "result = analyzer.analyze({\"source_data\": df})\n",
    "\n",
    "# Quick summary in console\n",
    "print(result.summary)\n",
    "\n",
    "# Optional: Save full report to Excel with multiple sheets\n",
    "result.to_excel(f\"dq_report{i+1}.xlsx\")"
   ],
   "id": "88ab9a7d39225d88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "         table  rows  cols  exact_row_dupes  columns_with_near_text_dupes  \\\n",
      "0  source_data  5347    60               10                            41   \n",
      "\n",
      "   columns_with_candidate_key_dupes  numeric_columns_with_outliers  \\\n",
      "0                                13                              1   \n",
      "\n",
      "   avg_null_pct  constant_columns  \n",
      "0     67.854917                21  \n"
     ]
    }
   ],
   "execution_count": 30
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "3cf4e31c18e96267"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
